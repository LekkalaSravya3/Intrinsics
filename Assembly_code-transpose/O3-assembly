	.file	"transpose.cpp"
	.text
	.p2align 4
	.globl	_Z16transpose_scalarPA8_KfPA8_f
	.type	_Z16transpose_scalarPA8_KfPA8_f, @function
_Z16transpose_scalarPA8_KfPA8_f:
.LFB7585:
	.cfi_startproc
	endbr64
	leaq	255(%rdi), %rax
	subq	%rsi, %rax
	cmpq	$510, %rax
	jbe	.L2
	vmovups	32(%rdi), %ymm7
	vmovups	(%rdi), %ymm4
	vmovups	96(%rdi), %ymm8
	vmovups	64(%rdi), %ymm0
	vshufps	$136, %ymm7, %ymm4, %ymm6
	vperm2f128	$3, %ymm6, %ymm6, %ymm5
	vshufps	$221, %ymm7, %ymm4, %ymm4
	vmovups	160(%rdi), %ymm10
	vshufps	$68, %ymm5, %ymm6, %ymm2
	vshufps	$238, %ymm5, %ymm6, %ymm5
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vmovups	128(%rdi), %ymm3
	vinsertf128	$1, %xmm5, %ymm2, %ymm2
	vshufps	$68, %ymm6, %ymm4, %ymm5
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vshufps	$136, %ymm8, %ymm0, %ymm6
	vperm2f128	$3, %ymm6, %ymm6, %ymm4
	vshufps	$221, %ymm8, %ymm0, %ymm0
	vmovups	224(%rdi), %ymm9
	vshufps	$68, %ymm4, %ymm6, %ymm7
	vshufps	$238, %ymm4, %ymm6, %ymm4
	vinsertf128	$1, %xmm4, %ymm7, %ymm7
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vmovups	192(%rdi), %ymm1
	vshufps	$136, %ymm10, %ymm3, %ymm6
	vshufps	$68, %ymm4, %ymm0, %ymm8
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vinsertf128	$1, %xmm4, %ymm8, %ymm8
	vperm2f128	$3, %ymm6, %ymm6, %ymm4
	vshufps	$221, %ymm10, %ymm3, %ymm3
	vshufps	$68, %ymm4, %ymm6, %ymm0
	vshufps	$238, %ymm4, %ymm6, %ymm4
	vperm2f128	$3, %ymm3, %ymm3, %ymm6
	vshufps	$136, %ymm9, %ymm1, %ymm10
	vinsertf128	$1, %xmm4, %ymm0, %ymm0
	vshufps	$68, %ymm6, %ymm3, %ymm4
	vshufps	$238, %ymm6, %ymm3, %ymm6
	vperm2f128	$3, %ymm10, %ymm10, %ymm3
	vshufps	$221, %ymm9, %ymm1, %ymm1
	vinsertf128	$1, %xmm6, %ymm4, %ymm4
	vshufps	$68, %ymm3, %ymm10, %ymm6
	vshufps	$238, %ymm3, %ymm10, %ymm3
	vinsertf128	$1, %xmm3, %ymm6, %ymm6
	vperm2f128	$3, %ymm1, %ymm1, %ymm3
	vshufps	$136, %ymm7, %ymm2, %ymm10
	vshufps	$221, %ymm7, %ymm2, %ymm2
	vshufps	$68, %ymm3, %ymm1, %ymm9
	vperm2f128	$3, %ymm2, %ymm2, %ymm7
	vshufps	$238, %ymm3, %ymm1, %ymm3
	vperm2f128	$3, %ymm10, %ymm10, %ymm1
	vinsertf128	$1, %xmm3, %ymm9, %ymm9
	vshufps	$68, %ymm1, %ymm10, %ymm3
	vshufps	$238, %ymm1, %ymm10, %ymm1
	vshufps	$136, %ymm6, %ymm0, %ymm10
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$68, %ymm7, %ymm2, %ymm1
	vshufps	$238, %ymm7, %ymm2, %ymm7
	vperm2f128	$3, %ymm10, %ymm10, %ymm2
	vshufps	$221, %ymm6, %ymm0, %ymm0
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vshufps	$68, %ymm2, %ymm10, %ymm7
	vshufps	$238, %ymm2, %ymm10, %ymm2
	vinsertf128	$1, %xmm2, %ymm7, %ymm7
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$136, %ymm8, %ymm5, %ymm10
	vshufps	$221, %ymm8, %ymm5, %ymm5
	vshufps	$68, %ymm2, %ymm0, %ymm6
	vperm2f128	$3, %ymm5, %ymm5, %ymm8
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vperm2f128	$3, %ymm10, %ymm10, %ymm0
	vinsertf128	$1, %xmm2, %ymm6, %ymm6
	vshufps	$68, %ymm0, %ymm10, %ymm2
	vshufps	$238, %ymm0, %ymm10, %ymm0
	vshufps	$136, %ymm9, %ymm4, %ymm10
	vinsertf128	$1, %xmm0, %ymm2, %ymm2
	vshufps	$68, %ymm8, %ymm5, %ymm0
	vshufps	$238, %ymm8, %ymm5, %ymm8
	vinsertf128	$1, %xmm8, %ymm0, %ymm0
	vperm2f128	$3, %ymm10, %ymm10, %ymm8
	vshufps	$221, %ymm9, %ymm4, %ymm4
	vshufps	$68, %ymm8, %ymm10, %ymm5
	vshufps	$238, %ymm8, %ymm10, %ymm8
	vinsertf128	$1, %xmm8, %ymm5, %ymm5
	vperm2f128	$3, %ymm4, %ymm4, %ymm8
	vshufps	$68, %ymm8, %ymm4, %ymm9
	vshufps	$238, %ymm8, %ymm4, %ymm8
	vinsertf128	$1, %xmm8, %ymm9, %ymm4
	vshufps	$136, %ymm7, %ymm3, %ymm9
	vperm2f128	$3, %ymm9, %ymm9, %ymm8
	vshufps	$221, %ymm7, %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm7
	vshufps	$68, %ymm8, %ymm9, %ymm10
	vshufps	$238, %ymm8, %ymm9, %ymm8
	vinsertf128	$1, %xmm8, %ymm10, %ymm8
	vshufps	$136, %ymm5, %ymm2, %ymm9
	vmovups	%ymm8, (%rsi)
	vperm2f128	$3, %ymm9, %ymm9, %ymm8
	vshufps	$221, %ymm5, %ymm2, %ymm2
	vshufps	$68, %ymm8, %ymm9, %ymm10
	vshufps	$238, %ymm8, %ymm9, %ymm8
	vinsertf128	$1, %xmm8, %ymm10, %ymm8
	vshufps	$136, %ymm6, %ymm1, %ymm9
	vmovups	%ymm8, 32(%rsi)
	vperm2f128	$3, %ymm9, %ymm9, %ymm8
	vshufps	$221, %ymm6, %ymm1, %ymm1
	vshufps	$68, %ymm8, %ymm9, %ymm10
	vshufps	$238, %ymm8, %ymm9, %ymm8
	vinsertf128	$1, %xmm8, %ymm10, %ymm8
	vshufps	$136, %ymm4, %ymm0, %ymm9
	vmovups	%ymm8, 64(%rsi)
	vperm2f128	$3, %ymm9, %ymm9, %ymm8
	vshufps	$221, %ymm4, %ymm0, %ymm0
	vshufps	$68, %ymm8, %ymm9, %ymm10
	vshufps	$238, %ymm8, %ymm9, %ymm8
	vinsertf128	$1, %xmm8, %ymm10, %ymm8
	vmovups	%ymm8, 96(%rsi)
	vshufps	$68, %ymm7, %ymm3, %ymm8
	vshufps	$238, %ymm7, %ymm3, %ymm7
	vinsertf128	$1, %xmm7, %ymm8, %ymm3
	vmovups	%ymm3, 128(%rsi)
	vperm2f128	$3, %ymm2, %ymm2, %ymm3
	vshufps	$68, %ymm3, %ymm2, %ymm5
	vshufps	$238, %ymm3, %ymm2, %ymm3
	vinsertf128	$1, %xmm3, %ymm5, %ymm2
	vmovups	%ymm2, 160(%rsi)
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm1
	vmovups	%ymm1, 192(%rsi)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm2, %ymm0
	vmovups	%ymm0, 224(%rsi)
	vzeroupper
	ret
.L2:
	vmovss	(%rdi), %xmm0
	vmovss	%xmm0, (%rsi)
	vmovss	4(%rdi), %xmm0
	vmovss	%xmm0, 32(%rsi)
	vmovss	8(%rdi), %xmm0
	vmovss	%xmm0, 64(%rsi)
	vmovss	12(%rdi), %xmm0
	vmovss	%xmm0, 96(%rsi)
	vmovss	16(%rdi), %xmm0
	vmovss	%xmm0, 128(%rsi)
	vmovss	20(%rdi), %xmm0
	vmovss	%xmm0, 160(%rsi)
	vmovss	24(%rdi), %xmm0
	vmovss	%xmm0, 192(%rsi)
	vmovss	28(%rdi), %xmm0
	vmovss	%xmm0, 224(%rsi)
	vmovss	32(%rdi), %xmm0
	vmovss	%xmm0, 4(%rsi)
	vmovss	36(%rdi), %xmm0
	vmovss	%xmm0, 36(%rsi)
	vmovss	40(%rdi), %xmm0
	vmovss	%xmm0, 68(%rsi)
	vmovss	44(%rdi), %xmm0
	vmovss	%xmm0, 100(%rsi)
	vmovss	48(%rdi), %xmm0
	vmovss	%xmm0, 132(%rsi)
	vmovss	52(%rdi), %xmm0
	vmovss	%xmm0, 164(%rsi)
	vmovss	56(%rdi), %xmm0
	vmovss	%xmm0, 196(%rsi)
	vmovss	60(%rdi), %xmm0
	vmovss	%xmm0, 228(%rsi)
	vmovss	64(%rdi), %xmm0
	vmovss	%xmm0, 8(%rsi)
	vmovss	68(%rdi), %xmm0
	vmovss	%xmm0, 40(%rsi)
	vmovss	72(%rdi), %xmm0
	vmovss	%xmm0, 72(%rsi)
	vmovss	76(%rdi), %xmm0
	vmovss	%xmm0, 104(%rsi)
	vmovss	80(%rdi), %xmm0
	vmovss	%xmm0, 136(%rsi)
	vmovss	84(%rdi), %xmm0
	vmovss	%xmm0, 168(%rsi)
	vmovss	88(%rdi), %xmm0
	vmovss	%xmm0, 200(%rsi)
	vmovss	92(%rdi), %xmm0
	vmovss	%xmm0, 232(%rsi)
	vmovss	96(%rdi), %xmm0
	vmovss	%xmm0, 12(%rsi)
	vmovss	100(%rdi), %xmm0
	vmovss	%xmm0, 44(%rsi)
	vmovss	104(%rdi), %xmm0
	vmovss	%xmm0, 76(%rsi)
	vmovss	108(%rdi), %xmm0
	vmovss	%xmm0, 108(%rsi)
	vmovss	112(%rdi), %xmm0
	vmovss	%xmm0, 140(%rsi)
	vmovss	116(%rdi), %xmm0
	vmovss	%xmm0, 172(%rsi)
	vmovss	120(%rdi), %xmm0
	vmovss	%xmm0, 204(%rsi)
	vmovss	124(%rdi), %xmm0
	vmovss	%xmm0, 236(%rsi)
	vmovss	128(%rdi), %xmm0
	vmovss	%xmm0, 16(%rsi)
	vmovss	132(%rdi), %xmm0
	vmovss	%xmm0, 48(%rsi)
	vmovss	136(%rdi), %xmm0
	vmovss	%xmm0, 80(%rsi)
	vmovss	140(%rdi), %xmm0
	vmovss	%xmm0, 112(%rsi)
	vmovss	144(%rdi), %xmm0
	vmovss	%xmm0, 144(%rsi)
	vmovss	148(%rdi), %xmm0
	vmovss	%xmm0, 176(%rsi)
	vmovss	152(%rdi), %xmm0
	vmovss	%xmm0, 208(%rsi)
	vmovss	156(%rdi), %xmm0
	vmovss	%xmm0, 240(%rsi)
	vmovss	160(%rdi), %xmm0
	vmovss	%xmm0, 20(%rsi)
	vmovss	164(%rdi), %xmm0
	vmovss	%xmm0, 52(%rsi)
	vmovss	168(%rdi), %xmm0
	vmovss	%xmm0, 84(%rsi)
	vmovss	172(%rdi), %xmm0
	vmovss	%xmm0, 116(%rsi)
	vmovss	176(%rdi), %xmm0
	vmovss	%xmm0, 148(%rsi)
	vmovss	180(%rdi), %xmm0
	vmovss	%xmm0, 180(%rsi)
	vmovss	184(%rdi), %xmm0
	vmovss	%xmm0, 212(%rsi)
	vmovss	188(%rdi), %xmm0
	vmovss	%xmm0, 244(%rsi)
	vmovss	192(%rdi), %xmm0
	vmovss	%xmm0, 24(%rsi)
	vmovss	196(%rdi), %xmm0
	vmovss	%xmm0, 56(%rsi)
	vmovss	200(%rdi), %xmm0
	vmovss	%xmm0, 88(%rsi)
	vmovss	204(%rdi), %xmm0
	vmovss	%xmm0, 120(%rsi)
	vmovss	208(%rdi), %xmm0
	vmovss	%xmm0, 152(%rsi)
	vmovss	212(%rdi), %xmm0
	vmovss	%xmm0, 184(%rsi)
	vmovss	216(%rdi), %xmm0
	vmovss	%xmm0, 216(%rsi)
	vmovss	220(%rdi), %xmm0
	vmovss	%xmm0, 248(%rsi)
	vmovss	224(%rdi), %xmm0
	vmovss	%xmm0, 28(%rsi)
	vmovss	228(%rdi), %xmm0
	vmovss	%xmm0, 60(%rsi)
	vmovss	232(%rdi), %xmm0
	vmovss	%xmm0, 92(%rsi)
	vmovss	236(%rdi), %xmm0
	vmovss	%xmm0, 124(%rsi)
	vmovss	240(%rdi), %xmm0
	vmovss	%xmm0, 156(%rsi)
	vmovss	244(%rdi), %xmm0
	vmovss	%xmm0, 188(%rsi)
	vmovss	248(%rdi), %xmm0
	vmovss	%xmm0, 220(%rsi)
	vmovss	252(%rdi), %xmm0
	vmovss	%xmm0, 252(%rsi)
	ret
	.cfi_endproc
.LFE7585:
	.size	_Z16transpose_scalarPA8_KfPA8_f, .-_Z16transpose_scalarPA8_KfPA8_f
	.p2align 4
	.globl	_Z17transpose_avx_8x8PA8_KfPA8_f
	.type	_Z17transpose_avx_8x8PA8_KfPA8_f, @function
_Z17transpose_avx_8x8PA8_KfPA8_f:
.LFB7586:
	.cfi_startproc
	endbr64
	vmovups	32(%rdi), %ymm4
	vmovups	96(%rdi), %ymm2
	vmovups	160(%rdi), %ymm5
	vmovups	224(%rdi), %ymm8
	vmovups	(%rdi), %ymm6
	vmovups	64(%rdi), %ymm0
	vmovups	128(%rdi), %ymm1
	vmovups	192(%rdi), %ymm3
	vunpcklps	%ymm4, %ymm6, %ymm7
	vunpckhps	%ymm4, %ymm6, %ymm6
	vunpcklps	%ymm2, %ymm0, %ymm4
	vunpckhps	%ymm2, %ymm0, %ymm0
	vunpcklps	%ymm5, %ymm1, %ymm2
	vunpckhps	%ymm5, %ymm1, %ymm1
	vunpcklps	%ymm8, %ymm3, %ymm5
	vunpckhps	%ymm8, %ymm3, %ymm3
	vshufps	$68, %ymm4, %ymm7, %ymm8
	vshufps	$238, %ymm4, %ymm7, %ymm4
	vshufps	$68, %ymm0, %ymm6, %ymm7
	vshufps	$238, %ymm0, %ymm6, %ymm0
	vshufps	$68, %ymm5, %ymm2, %ymm6
	vshufps	$238, %ymm5, %ymm2, %ymm2
	vinsertf128	$1, %xmm6, %ymm8, %ymm12
	vshufps	$68, %ymm3, %ymm1, %ymm5
	vinsertf128	$1, %xmm2, %ymm4, %ymm11
	vshufps	$238, %ymm3, %ymm1, %ymm1
	vperm2f128	$49, %ymm2, %ymm4, %ymm4
	vinsertf128	$1, %xmm1, %ymm0, %ymm9
	vmovups	%ymm12, (%rsi)
	vinsertf128	$1, %xmm5, %ymm7, %ymm10
	vperm2f128	$49, %ymm6, %ymm8, %ymm3
	vperm2f128	$49, %ymm5, %ymm7, %ymm2
	vperm2f128	$49, %ymm1, %ymm0, %ymm0
	vmovups	%ymm11, 32(%rsi)
	vmovups	%ymm10, 64(%rsi)
	vmovups	%ymm9, 96(%rsi)
	vmovups	%ymm3, 128(%rsi)
	vmovups	%ymm4, 160(%rsi)
	vmovups	%ymm2, 192(%rsi)
	vmovups	%ymm0, 224(%rsi)
	vzeroupper
	ret
	.cfi_endproc
